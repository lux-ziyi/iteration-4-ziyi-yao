import findspark
findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')
import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('basics').getOrCreate()
df = spark.read.csv('./master.csv', header=True, inferSchema=True)
print((df.count(), len(df.columns)))
df.columns
df.show()
df.describe().show()
df.printSchema()
!pip3 install seaborn
import seaborn as sns
import matplotlib.pyplot as plt
pandas_df = df.toPandas()
plt.figure(figsize=(12, 16))
plt.subplot(311)
sns.lineplot(x='year', y='suicides/100k pop', hue='sex', data=pandas_df, palette="hot")  
plt.xticks(ha='right', fontsize=20)
plt.ylabel('suicides/100k pop', fontsize=20)
plt.xlabel('year', fontsize=20)
plt.legend(fontsize=14, loc='best')  
plt.title("Relationship between sex year and suicide rate")
plt.show()
sns.set_color_codes('muted')
sns.barplot(x='age',y='suicides/100k pop',data=pandas_df)
plt.xticks(rotation=90)
plt.show()
from pyspark.sql.functions import isnull, when, count, col
df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()
from pyspark.sql.functions import *
df = df.withColumn(' gdp_for_year ($) ', regexp_replace(' gdp_for_year ($) ', ',', '').cast('float'))
df.printSchema()
columns_to_drop = ['country-year', 'suicides_no']
df = df.drop(*columns_to_drop)
df.columns
df=df.withColumn('population of country', df[' gdp_for_year ($) ']/df['gdp_per_capita ($)']) 
df.columns

from pyspark.sql.functions import mean
mean_HDI = df.select(mean(df['HDI for year'])).collect()
mean_HDI
mean_HDI[0]
mean_HDI[0][0]
mean_HDI_val = mean_HDI[0][0]
df=df.na.fill(mean_HDI_val,subset=['HDI for year'])

dataset= df.toPandas()
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
dataset['country']= le.fit_transform(dataset['country'])  
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
dataset['sex']= le.fit_transform(dataset['sex']) 
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
dataset['age']= le.fit_transform(dataset['age']) 
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
dataset['generation']= le.fit_transform(dataset['generation']) 


print(dataset['country'])
print(dataset['sex'])
print(dataset['age'])
print(dataset['generation'])
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
print(scaler.fit_transform(dataset))


import findspark
findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')
import pyspark
from pyspark.sql import SparkSession


spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()
from pyspark.sql.types import *
dataset = spark.createDataFrame(dataset)

type(dataset)



from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('tree_methods_doc').getOrCreate()


from pyspark.ml.feature import VectorAssembler
featureArray = ['year', 'sex', 'age','population','HDI for year',
 ' gdp_for_year ($) ','gdp_per_capita ($)','country','population of country','generation']
df_assembler = VectorAssembler(inputCols=featureArray, outputCol="feature")

df = df_assembler.transform(dataset)
df.printSchema()

df.select(['feature','suicides/100k pop']).show(10,False)

from pyspark.ml.regression import RandomForestRegressor
rf = RandomForestRegressor(labelCol="suicides/100k pop", featuresCol='feature', numTrees=20)

model_df= rf.fit(df)
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
FI = pd.Series(model_df.featureImportances,index=featureArray) # pySpark

FI = FI.sort_values(ascending = False)
fig = plt.figure(figsize=(12,5))
plt.bar(FI.index,FI.values,color="blue")
plt.xlabel('feature')
plt.ylabel('importances')
plt.show()

ff=model_df.featureImportances
importancesList=[float(col) for col in  ff]
colList=featureArray
result=dict(zip(colList,importancesList))
print(result)

columns_to_drop = ['HDI for year']
df = df.drop(*columns_to_drop)
df.columns

from pyspark.ml.feature import VectorAssembler
featureArray = ['year', 'sex', 'age','population',' gdp_for_year ($) ',
                'gdp_per_capita ($)','country','population of country']
df_assembler = VectorAssembler(inputCols=featureArray, outputCol="features")

df = df_assembler.transform(df)
df.printSchema()

df.select(['features','suicides/100k pop']).show(10,False)


model_df=df.select(['features','suicides/100k pop'])                                         
train_df,test_df=model_df.randomSplit([0.80,0.20])                                  

train_df.describe().show()
test_df.describe().show()


from pyspark.ml.regression import RandomForestRegressor
rf = RandomForestRegressor(labelCol="suicides/100k pop", featuresCol='features', numTrees=20)

model_train_rf= rf.fit(train_df)
model_test_rf = rf.fit(test_df)
prediction_train_rf=model_train_rf.transform(train_df)
prediction_test_rf=model_test_rf.transform(test_df)

from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator(labelCol="suicides/100k pop", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(prediction_train_rf)
print("Root Mean Squared Error (RMSE) on train data = %g" % rmse)
rmse1 = evaluator.evaluate(prediction_test_rf)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse1)


import matplotlib as mpl
import matplotlib.pyplot as plt

FI = pd.Series(model_train_rf.featureImportances,index=featureArray) # pySpark

FI = FI.sort_values(ascending = False)
fig = plt.figure(figsize=(12,5))
plt.bar(FI.index,FI.values,color="blue")
plt.xlabel('features')
plt.ylabel('importances')
plt.show()

ff=model_train_rf.featureImportances
importancesList=[float(col) for col in  ff]
colList=featureArray
result=dict(zip(colList,importancesList))
print(result)










































